{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAk5NakYtPpA/Hi/AZMgbd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BI-DS/ELE-3909/blob/master/lecture6/news_articles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWSvQdazA4zI",
        "outputId": "4713d781-1925-47ec-d1bf-8976d51d876d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import os\n",
        "from sklearn.manifold import TSNE\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transforming news articles to TF-IDF embeddings"
      ],
      "metadata": {
        "id": "uiQQccxNgVP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First define a function to clean-up the raw text"
      ],
      "metadata": {
        "id": "qcyg3jrMgfda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    if type(text) == float:\n",
        "        return \"\"\n",
        "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "    normalMap = {'í':'i', 'ó':'o', 'á':'a', 'é':'e', 'ú':'u', 'ñ':'n'}\n",
        "    normalize = str.maketrans(normalMap)\n",
        "    temp = text.lower()\n",
        "    temp = temp.translate(normalize)\n",
        "\n",
        "    temp = re.sub(\"'\", \"\", temp) # to avoid removing contractions in english\n",
        "    temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp)\n",
        "    temp = re.sub(r'http\\S+', '', temp)\n",
        "    temp = re.sub('[()!?]', ' ', temp)\n",
        "    temp = re.sub('\\[.*?\\]',' ', temp)\n",
        "    temp = re.sub(\"[^a-z0-9]\",\" \", temp)\n",
        "    temp = temp.split()\n",
        "    temp = [w for w in temp if not w in stopwords]\n",
        "    temp = \" \".join(word for word in temp)\n",
        "    return temp"
      ],
      "metadata": {
        "id": "GHWwIOkGBG3e"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the txt file from github ..."
      ],
      "metadata": {
        "id": "rfItgdpSgktJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P . https://raw.githubusercontent.com/BI-DS/ELE-3909/master/lecture6/news_articles.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1WUwpQgDY4p",
        "outputId": "07f7d58c-7efd-4328-f33f-ffdeaf0795f5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-09 19:38:52--  https://raw.githubusercontent.com/BI-DS/ELE-3909/master/lecture6/news_articles.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25337978 (24M) [text/plain]\n",
            "Saving to: ‘./news_articles.txt’\n",
            "\n",
            "news_articles.txt   100%[===================>]  24.16M  50.3MB/s    in 0.5s    \n",
            "\n",
            "2023-10-09 19:38:54 (50.3 MB/s) - ‘./news_articles.txt’ saved [25337978/25337978]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can clean each of the texts and create a corpus...."
      ],
      "metadata": {
        "id": "EyAc17Jsgrdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_corpus():\n",
        "    with open(\"./news_articles.txt\", \"r\") as infile:\n",
        "        lines = infile.readlines()\n",
        "    print('total no of lines {}'.format(len(lines)))\n",
        "\n",
        "    lengths = []\n",
        "    corpus  = []\n",
        "    counter=0\n",
        "\n",
        "    if os.path.exists('./clean_text.txt'):\n",
        "      print('deleting old file...')\n",
        "      os.system('rm ./clean_text.txt')\n",
        "\n",
        "    with open(\"./clean_text.txt\", \"w\") as x:\n",
        "        for i, text in enumerate(lines):\n",
        "            clean_content = clean_text(text)\n",
        "            tokens = word_tokenize(clean_content)\n",
        "            length = len(tokens)\n",
        "\n",
        "            if length <= 100:\n",
        "                counter+=1\n",
        "                x.write(\" \".join(tokens)+\"\\n\")\n",
        "                corpus.append(\" \".join(tokens))\n",
        "                lengths.append(length)\n",
        "\n",
        "    print('{} news with length smaller than {}'.format(counter, np.max(lengths)))\n",
        "    print('done!')\n",
        "    x.close()\n",
        "\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "DW9tp3LXA_u1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = text_to_corpus()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6QV5CMjDKp8",
        "outputId": "3230b7bc-57a3-4fae-a4df-988b953d3893"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total no of lines 4551\n",
            "127 news with length smaller than 100\n",
            "done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate the TF-IDF using `sklearn`"
      ],
      "metadata": {
        "id": "LY39m7dSgz0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 50\n",
        "vectorizer  = TfidfVectorizer(max_features = max_features)\n",
        "tf_idf = vectorizer.fit_transform(corpus).toarray()"
      ],
      "metadata": {
        "id": "O3antuhbKg0M"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf_idf)"
      ],
      "metadata": {
        "id": "p6uVRqpmfsiP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27b28eef-09cb-4b9e-e700-d87420db97d2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.21764182 0.         0.33072754 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.53065399 0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.18349951 0.         0.         ... 0.         0.24365584 0.        ]]\n"
          ]
        }
      ]
    }
  ]
}