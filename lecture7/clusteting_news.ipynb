{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKBXqFBXvaloqbxLlszOiE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BI-DS/ELE-3909/blob/master/lecture7/clusteting_news.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVmsWaIFsncW",
        "outputId": "6b04fb5d-26ba-40a1-d004-5cf90b73262a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import os\n",
        "from sklearn.manifold import TSNE\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from bokeh.plotting import figure, output_file, save, gridplot, show\n",
        "from bokeh.transform import jitter\n",
        "from bokeh.models import HoverTool\n",
        "from bokeh.palettes import Category20_10 as Palette\n",
        "from bokeh.models import ColumnDataSource,OpenURL, TapTool\n",
        "from sklearn.manifold import TSNE\n",
        "from bokeh.transform import factor_cmap\n",
        "import pandas as pd\n",
        "import bokeh.io\n",
        "bokeh.io.output_notebook()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering news articles ðŸ”¥ðŸ”¥ðŸ”¥\n",
        "\n",
        "Get some articles from the course repository"
      ],
      "metadata": {
        "id": "rzrOhcvl_9Ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P . https://raw.githubusercontent.com/BI-DS/ELE-3909/master/lecture6/news_articles.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIvpPzmNtfhQ",
        "outputId": "fe276a74-db69-4ee9-9ece-51a96c65438e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-09 20:03:32--  https://raw.githubusercontent.com/BI-DS/ELE-3909/master/lecture6/news_articles.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25337978 (24M) [text/plain]\n",
            "Saving to: â€˜./news_articles.txtâ€™\n",
            "\n",
            "news_articles.txt   100%[===================>]  24.16M   158MB/s    in 0.2s    \n",
            "\n",
            "2023-10-09 20:03:32 (158 MB/s) - â€˜./news_articles.txtâ€™ saved [25337978/25337978]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define some functions to clean the data"
      ],
      "metadata": {
        "id": "lc26ILE6AXOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    if type(text) == float:\n",
        "        return \"\"\n",
        "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "    normalMap = {'Ã­':'i', 'Ã³':'o', 'Ã¡':'a', 'Ã©':'e', 'Ãº':'u', 'Ã±':'n'}\n",
        "    normalize = str.maketrans(normalMap)\n",
        "    temp = text.lower()\n",
        "    temp = temp.translate(normalize)\n",
        "\n",
        "    temp = re.sub(\"'\", \"\", temp) # to avoid removing contractions in english\n",
        "    temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp)\n",
        "    temp = re.sub(r'http\\S+', '', temp)\n",
        "    temp = re.sub('[()!?]', ' ', temp)\n",
        "    temp = re.sub('\\[.*?\\]',' ', temp)\n",
        "    temp = re.sub(\"[^a-z0-9]\",\" \", temp)\n",
        "    temp = temp.split()\n",
        "    temp = [w for w in temp if not w in stopwords]\n",
        "    temp = \" \".join(word for word in temp)\n",
        "    return temp"
      ],
      "metadata": {
        "id": "CWA2rkNWtvEB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below is almost as the one we saw last lecture. This version, however, keeps track of the index in the original file with all news"
      ],
      "metadata": {
        "id": "HWEehz9GAbkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_corpus():\n",
        "    with open(\"./news_articles.txt\", \"r\") as infile:\n",
        "        lines = infile.readlines()\n",
        "    print('total no of lines {}'.format(len(lines)))\n",
        "\n",
        "    lengths = []\n",
        "    corpus  = []\n",
        "    orig_idx = []\n",
        "    counter=0\n",
        "\n",
        "    if os.path.exists('./clean_text.txt'):\n",
        "      print('deleting old file...')\n",
        "      os.system('rm ./clean_text.txt')\n",
        "\n",
        "    with open(\"./clean_text.txt\", \"w\") as x:\n",
        "        for i, text in enumerate(lines):\n",
        "            clean_content = clean_text(text)\n",
        "            tokens = word_tokenize(clean_content)\n",
        "            length = len(tokens)\n",
        "\n",
        "            if length <= 100:\n",
        "                orig_idx.append(i+1)\n",
        "                counter+=1\n",
        "                x.write(\" \".join(tokens)+\"\\n\")\n",
        "                corpus.append(\" \".join(tokens))\n",
        "                lengths.append(length)\n",
        "\n",
        "\n",
        "    print('{} news with length smaller than {}'.format(counter, np.max(lengths)))\n",
        "    print('done!')\n",
        "    x.close()\n",
        "\n",
        "    return corpus, orig_idx"
      ],
      "metadata": {
        "id": "kGEmxEZ4toR7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get corpus and original index for each article with less than 100 tokens"
      ],
      "metadata": {
        "id": "PvSolQK4AyLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus, orig_idx = text_to_corpus()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxHd9LXYtrKC",
        "outputId": "92189ca6-0de3-4a3b-8389-01b477e71582"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total no of lines 4551\n",
            "127 news with length smaller than 100\n",
            "done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cherry-pick some articles"
      ],
      "metadata": {
        "id": "UBft8ltRA8Up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c1 = [94,64,44,93,53,10,70,107,54,45,35]\n",
        "c2 = [66,95,50,87,60,65,76,13,40,43,36]\n",
        "c3 = [51,81,79,99,83,124,4,61,25,91,115]\n",
        "c_idx = [c1,c2,c3]\n",
        "clusters = []\n",
        "for i in range(127):\n",
        "  if i in c1:\n",
        "    clusters.append(0)\n",
        "  elif i in c2:\n",
        "    clusters.append(1)\n",
        "  elif i in c3:\n",
        "    clusters.append(2)\n",
        "  else:\n",
        "    clusters.append(-1)\n",
        "clusters = np.array(clusters)"
      ],
      "metadata": {
        "id": "5gj37Y_ptF7s"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will plot our results using the `bokeh` library. If you dont know it, take a look at it!"
      ],
      "metadata": {
        "id": "Z_qGj4GJBUI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First create a datafrem to save the corpus and indexes (it is easier when we use `bokeh`"
      ],
      "metadata": {
        "id": "A4XhIfAZBV2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_corpus = pd.DataFrame(np.c_[corpus,orig_idx])"
      ],
      "metadata": {
        "id": "m2ascDH2tP0h"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now define some variables to plot and save our scatter plot"
      ],
      "metadata": {
        "id": "GwwwKVoGBjR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_file(\"./plot_and_news.html\")\n",
        "p = figure(title=\"Visualizing News\")\n",
        "p.title.text_font_size = '15pt'\n",
        "p.title.align = 'center'\n",
        "p.background_fill_color = \"gray\"\n",
        "p.background_fill_alpha = 0.35"
      ],
      "metadata": {
        "id": "neayJ6gvt3oQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use t-SNE to get 2D data ðŸ”¥"
      ],
      "metadata": {
        "id": "1DuSJNM3Bqe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 50\n",
        "vectorizer  = TfidfVectorizer(max_features = max_features)\n",
        "tf_idf = vectorizer.fit_transform(corpus).toarray()\n",
        "transformer = TSNE(n_components=2,learning_rate='auto',init='random',n_jobs=-1,random_state=1234)\n",
        "representations = transformer.fit_transform(tf_idf)"
      ],
      "metadata": {
        "id": "osCgLC1ZtQdA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally an interactive scatter plot Â© âœ¨"
      ],
      "metadata": {
        "id": "3EW0Hh4LByVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colors = ['blue','red','yellow']\n",
        "for l in range(3):\n",
        "  x = representations[clusters==l,0]\n",
        "  y = representations[clusters==l,1]\n",
        "  cluster = list(np.repeat(l,x.shape[0]))\n",
        "  news=df_corpus.iloc[clusters==l][0].values\n",
        "  index =df_corpus.iloc[clusters==l][1].values\n",
        "  source = ColumnDataSource(data=dict(x=x,y=y,index=index,news=list(news),cluster=cluster))\n",
        "  s = p.circle(x='x',y='y', size=6, line_color='black', source=source, fill_color=colors[l])\n",
        "  p.add_tools(HoverTool(renderers=[s],tooltips=[(\"index\", \"@index\"),(\"news\",\"@news\"),('cluster',\"@cluster\")]))"
      ],
      "metadata": {
        "id": "RnKI2NL_x2ls"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save(p)\n",
        "bokeh.io.show(p)"
      ],
      "metadata": {
        "id": "YBRIKIHBx5AA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}